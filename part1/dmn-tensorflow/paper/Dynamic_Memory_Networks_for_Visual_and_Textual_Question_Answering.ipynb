{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Memory Networks for Visual and Textual Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / 텐서팔로우 : 파트 1 - 텐서코드리뷰 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* Abstract\n",
    "* 1 Introduction\n",
    "* 2 Dynamic Memory Networks\n",
    "* 3 Improved Dynamic Memory Networks: DMN+\n",
    "     - 3.1. Input Module for Text QA\n",
    "     - 3.2. Input Module for VQA\n",
    "     - 3.3. The Episodic Memory Module\n",
    "* 4 Related Work\n",
    "* 5 Datasets\n",
    "    - 5.1. bAbI-10k\n",
    "    - 5.2. DAQUAR-ALL visual dataset\n",
    "    - 5.3. Visual Question Answering\n",
    "* 6 Experiments\n",
    "    - 6.1. Model Analysis\n",
    "    - 6.2. Comparison to state of the art using bAbI-10k\n",
    "    - 6.3. Comparison to state of the art using VQA\n",
    "* 7 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural network architectures with <font color=\"red\">memory</font> and <font color=\"red\">attention</font> mechanisms exhibit certain reasoning capabilities required for <font color=\"red\">question answering</font>.\n",
    "* Our new <font color=\"red\">DMN+</font> model improves the state of the art on both the <font color=\"red\">Visual Question Answering dataset</font> and the <font color=\"red\">bAbI-10k text question-answering dataset</font> <font color=\"blue\">without supporting fact supervision</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [3] <font color=\"red\">The future of Deep Learning for NLP: Dynamic Memory Networks </font> (in CS224d: Deep Learning for Natural Language Processing) - http://cs224d.stanford.edu/lectures/CS224d-Lecture17.pdf\n",
    "* [5] Implementing Dynamic memory networks - https://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/\n",
    "* [6] Playground for bAbI tasks - https://yerevann.github.io/2016/02/23/playground-for-babi-tasks/\n",
    "* [8] <font color=\"red\">Dynamic Memory Networks by YerevanNN Web Demo</font> - ([6]의 웹 데모) - http://yerevann.com/dmn-ui/#/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We analyze the DMN components, specifically \n",
    "    - the input module and \n",
    "    - memory module, to improve question answering. \n",
    "* We propose a new input module which \n",
    "    - uses a two level encoder with \n",
    "        - a sentence reader and \n",
    "        - input fusion layer \n",
    "            - to allow for information flow \n",
    "                - between sentences. \n",
    "* For the memory, we propose \n",
    "    - a modification to gated recurrent units (GRU) (Chung et al., 2014). \n",
    "    - The new GRU formulation \n",
    "        - incorporates attention gates that \n",
    "            - are computed using global knowledge over the facts. \n",
    "* Unlike before, the new DMN+ model \n",
    "    - does not require that supporting facts \n",
    "        - (i.e. the facts that are relevant for answering a particular question) \n",
    "        - are labeled during training. \n",
    "    - The model learns to select the important facts from a larger set.\n",
    "* In addition, we introduce a new input module to represent images.\n",
    "    - We show that the changes in the memory module that improved textual question answering also improve visual question answering. Both tasks are illustrated in Fig. 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dynamic Memory Networks\n",
    "* Input Module\n",
    "* Question Module\n",
    "* Episodic Memory Module\n",
    "* Answer Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/overview.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "* [10] GRUs and LSTMs -- for machine translation (in CS224d: Deep Learning for Natural Language Processing) - http://cs224d.stanford.edu/lectures/CS224d-Lecture9.pdf\n",
    "* [11] Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM) / RNN language models / Image captioning (in CS231n: Convolutional Neural Networks for Visual Recognition) - http://cs231n.stanford.edu/slides/winter1516_lecture10.pdf\n",
    "* [12] 엘에스티엠 네트워크 이해하기 - http://roboticist.tistory.com/m/post/571"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/input.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module processes the input data about which a question is being asked into a set of vectors termed <font color=\"red\">facts, represented as</font> $F = [f_1, . . . , f_N ]$ , where $N$ is the total number of facts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module computes a vector representation $q$ of the question, where $q ∈ R^{n_H}$ is the final hidden state of a GRU over the words in the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/query.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episodic Memory Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/episodic.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/episodic_repeat.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episode memory aims to retrieve the information required to answer the question $q$ from the input facts. \n",
    "* To improve our understanding of both the question and input, especially if questions require transitive reasoning, the episode memory module may pass over the input multiple times, updating episode memory after each pass.\n",
    "* We refer to \n",
    "    - the episode memory on the $t$th pass over the inputs as $m_t$, \n",
    "        - where $m_t ∈ R^{n_H}$ , \n",
    "    - the initial memory vector is set to the question vector: $m_0 = q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "* [13] Deep Learning for Computer Vision: Attention Models (UPC 2016) - http://www.slideshare.net/xavigiro/deep-learning-for-computer-vision-attention-models-upc-2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <font color=\"blue\">episodic memory module</font> consists of two separate components:\n",
    "* <font color=\"red\">The attention mechanism</font> \n",
    "    - is responsible for producing a contextual vector $c_t$, \n",
    "        - where $c_t ∈ R^{n_H}$ is a summary of relevant input for pass $t$, \n",
    "            - with relevance inferred by the question $q$\n",
    "            - and previous episode memory $m_{t−1}$. \n",
    "* <font color=\"red\">The memory update mechanism</font> \n",
    "    - is responsible for generating the episode memory $m_t$ \n",
    "        - based upon the contextual vector $c_t$ and \n",
    "        - previous episode memory $m_{t−1}$. \n",
    "    - By the final pass $T$ , \n",
    "        - the episodic memory $m_T$ \n",
    "            - should contain all the information \n",
    "                - required to answer the question $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer module receives both q and $m^T$ to generate the model’s predicted answer. \n",
    "* For simple answers,\n",
    "    - such as a single word, a linear layer with softmax activation may be used. \n",
    "* For tasks requiring a sequence output, \n",
    "    - an RNN may be used to decode $a = [q; m^T]$, \n",
    "        - the concatenation of vectors $q$ and $m^T$ , to an ordered set of tokens.\n",
    "* The cross entropy error on the answers is used for training and backpropagated through the entire network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/answer.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Improved Dynamic Memory Networks: DMN+\n",
    "* 3.1. Input Module for Text QA\n",
    "* 3.2. Input Module for VQA\n",
    "* 3.3. The Episodic Memory Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The final DMN+ model obtains the highest accuracy on the bAbI-10k dataset without supporting facts and the VQA dataset (Antol et al., 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Input Module for Text QA\n",
    "* Input Fusion Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DMN specified in Kumaretal.(2015),\n",
    "* a single GRU \n",
    "    - is used to process all the words in the story, extracting sentence representations by storing the hidden states produced at the end of sentence markers.\n",
    "    - The GRU also provides a temporal component by allowing a sentence to know the content of the sentences that came before them.\n",
    "* Whilst this input module \n",
    "    - worked well for bAbI-1k \n",
    "        - with supporting facts, as reported in Kumar et al. (2015), it did \n",
    "    - not perform well on bAbI-10k \n",
    "        - without supporting facts (Sec. 6.1)\n",
    "* We speculate that there are two main reasons for this performance disparity, all exacerbated by the removal of supporting facts. \n",
    "    * First, \n",
    "        - the GRU only allows sentences to have context from sentences before them, but not after them. \n",
    "            - This prevents information propagation from future sentences. \n",
    "    * Second, \n",
    "        - the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Fusion Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DMN+, we propose replacing this single GRU with two different components. \n",
    "* The first component is \n",
    "    - a <font color=\"red\">sentence reader</font>, \n",
    "        - responsible only for encoding the words into a sentence embedding. \n",
    "* The second component is \n",
    "    - the <font color=\"red\">input fusion layer</font>, \n",
    "        - allowing for interactions between sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence encoding \n",
    "* $f_i$ is the output of an encoding scheme \n",
    "    - taking the word tokens $[w^{i}_1 , . . . , w^{i}_{M_i}]$, \n",
    "        - where $M_i$ is the length of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### positional encoding scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 참고\n",
    "* [14] End-To-End Memory Networks - http://arxiv.org/abs/1503.08895"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence reader could be based on any variety of encoding schemes. We selected positional encoding described in Sukhbaatar et al. (2015) to allow for a comparison to their work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Input Module for VQA\n",
    "* Local region feature extraction\n",
    "* Visual feature embedding\n",
    "* Input fusion layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local region feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual feature embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input fusion layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. The Episodic Memory Module\n",
    "* Attention Mechanism\n",
    "* Soft attention\n",
    "* Attention based GRU\n",
    "* Episode Memory Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention based GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episode Memory Updates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Related Work\n",
    "* Neural Memory Models\n",
    "* Neural Attention Mechanisms \n",
    "* Question Answering in NLP\n",
    "* Visual Question Answering (VQA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Memory Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Attention Mechanisms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Answering in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Question Answering (VQA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Datasets\n",
    "* 5.1. bAbI-10k\n",
    "* 5.2. DAQUAR-ALL visual dataset\n",
    "* 5.3. Visual Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. bAbI-10k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. DAQUAR-ALL visual dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Visual Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Experiments\n",
    "* 6.1. Model Analysis\n",
    "* 6.2. Comparison to state of the art using bAbI-10k\n",
    "* 6.3. Comparison to state of the art using VQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Comparison to state of the art using bAbI-10k\n",
    "* Text QA Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text QA Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"figures/cap15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Comparison to state of the art using VQA\n",
    "* Training Details \n",
    "* Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Details "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"figures/cap16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] Dynamic Memory Networks for Visual and Textual Question Answering - https://arxiv.org/abs/1603.01417\n",
    "* [2] Ask Me Anything: Dynamic Memory Networks for Natural Language Processing - http://arxiv.org/abs/1506.07285\n",
    "* [3] The future of Deep Learning for NLP: Dynamic Memory Networks (in CS224d: Deep Learning for Natural Language Processing) - http://cs224d.stanford.edu/lectures/CS224d-Lecture17.pdf\n",
    "* [4] code(therne's) - https://github.com/therne/dmn-tensorflow\n",
    "* [5] Implementing Dynamic memory networks - https://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/\n",
    "* [6] Playground for bAbI tasks - https://yerevann.github.io/2016/02/23/playground-for-babi-tasks/\n",
    "* [7] code(YerevaNN's) - https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano\n",
    "* [8] Dynamic Memory Networks by YerevanNN Web Demo - ([6]의 웹 데모) - http://yerevann.com/dmn-ui/#/\n",
    "* [9] TOWARDS AI-COMPLETE QUESTION ANSWERING : A SET OF PREREQUISITE TOY TASKS -  http://arxiv.org/pdf/1502.05698v10.pdf\n",
    "* [10] GRUs and LSTMs -- for machine translation (in CS224d: Deep Learning for Natural Language Processing) - http://cs224d.stanford.edu/lectures/CS224d-Lecture9.pdf\n",
    "* [11] Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM) / RNN language models / Image captioning (in CS231n: Convolutional Neural Networks for Visual Recognition) - http://cs231n.stanford.edu/slides/winter1516_lecture10.pdf\n",
    "* [12] 엘에스티엠 네트워크 이해하기 - http://roboticist.tistory.com/m/post/571\n",
    "* [13] Deep Learning for Computer Vision: Attention Models (UPC 2016) - http://www.slideshare.net/xavigiro/deep-learning-for-computer-vision-attention-models-upc-2016\n",
    "* [14] End-To-End Memory Networks - http://arxiv.org/abs/1503.08895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
