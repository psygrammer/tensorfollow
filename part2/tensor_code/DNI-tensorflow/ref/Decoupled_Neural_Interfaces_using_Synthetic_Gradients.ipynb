{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoupled Neural Interfaces using Synthetic Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / DGM : 파트 1 - DeepMind 논문리뷰 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 1 Introduction\n",
    "* 2 Decoupled Neural Interfaces\n",
    "    - 2.1 Synthetic Gradient for Feed-Forward Networks\n",
    "    - 2.2 Synthetic Gradient for Recurrent Networks\n",
    "* 3 Experiments\n",
    "    - 3.1 Feed-Forward Networks\n",
    "    - 3.2 Recurrent Neural Networks\n",
    "    - 3.3 Multi-Network System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [1] (deepmind original paper) Decoupled Neural Interfaces using Synthetic Gradients - https://arxiv.org/pdf/1608.05343.pdf\n",
    "* [2] (deepmind blog) Decoupled Neural Interfaces using Synthetic Gradients - https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/\n",
    "* [3] (모두연) Decoupled Neural Interfaces using Synthetic Gradients - https://norman3.github.io/papers/docs/synthetic_gradients\n",
    "* [4] (code) Image classification with synthetic gradient in tensorflow - https://github.com/andrewliao11/DNI-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each layer (or module) in a directed neural network can be considered a computation step, that transforms its incoming data. \n",
    "* These modules are connected via directed edges, creating a forward processing graph which defines the flow of data from the network inputs, through each module, producing network outputs. \n",
    "* Defining a loss on outputs allows errors to be generated, and propagated back through the network graph to provide a signal to update each module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/KonanAcademy/deep/blob/master/seminar/season01/ch06/figures/cap6.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/KonanAcademy/deep/blob/master/seminar/season01/ch06/figures/cap6.81.png\" width=600 />\n",
    "<img src=\"http://nbviewer.jupyter.org/github/KonanAcademy/deep/blob/master/seminar/season01/ch06/figures/cap6.82.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/KonanAcademy/deep/blob/master/seminar/season01/ch06/figures/chain.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### locking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms.google.com.a.appspot.com/images/3-1.width-1500.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process results in several forms of locking, namely:\n",
    "* (i) Forward Locking \n",
    "    - no module can process its incoming data before the previous nodes in the directed forward graph have executed; \n",
    "* (ii) Update Locking \n",
    "    - no module can be updated before all dependent modules have executed in forwards mode; also, in many credit-assignment algorithms (including backpropagation [18]) we have \n",
    "* (iii) Backwards Locking  \n",
    "    - no module can be updated before all dependent modules have executed in both forwards mode and backwards mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forwards, update, and backwards locking constrains us to running and updating neural networks in a sequential, synchronous manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though seemingly benign when training simple feed-forward nets, this poses problems when thinking about creating systems of networks acting in multiple environments at different and possibly irregular or asynchronous timescales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms.google.com.a.appspot.com/images/3-2.width-1500.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To remove update locking for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://sebastianraschka.com/images/faq/visual-backpropagation/forward-propagation.png\" width=400 />\n",
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms.google.com.a.appspot.com/images/3-1.width-1500.png\" width=400 />\n",
    "<img src=\"http://sebastianraschka.com/images/faq/visual-backpropagation/backpropagation.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this work is to remove update locking for neural networks. This is achieved by removing backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update weights $θ_i$ of module $i$ we drastically approximate the function implied by backpropagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### synthetic gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, <font color=\"red\">we remove the reliance on backpropagation to get error gradients</font>, and <font color=\"blue\">instead learn a parametric model which predicts</font> what the gradients will be based upon only local information. We call these predicted gradients synthetic gradients.[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms.google.com.a.appspot.com/images/3-3.width-1500.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms.google.com.a.appspot.com/images/3-4.width-1500.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Decoupled Neural Interfaces\n",
    "* 2.1 Synthetic Gradient for Feed-Forward Networks\n",
    "* 2.2 Synthetic Gradient for Recurrent Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### communication protocol & update decoupled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Figure 1. (a) - General communication protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNI(Decoupled Neural Interfaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this protocol to neural networks communicating, resulting in what we call Decoupled Neural Interfaces (DNI). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms.google.com.a.appspot.com/images/3-5.width-1500.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms.google.com.a.appspot.com/documents/3-6.gif\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### synthetic gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we concentrate our empirical study on differentiable networks trained with backpropagation and gradient-based updates. Therefore, we focus on producing error gradients as the feedback $δ^{ˆ}_A$ which we dub synthetic gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Synthetic Gradient for Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Figure 1. (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/bp.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update decoupled & train synthetic gradient model('s paramter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Figure 1. (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Figure 1. (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, for a feed-forward network, we can use synthetic gradients as communication feedback to decouple every layer in the network, as shown in Fig. 1 (d). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The execution of this process is illustrated in Fig. 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms.google.com.a.appspot.com/documents/3-6.gif\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process allows each layer to be updated as soon as a forward pass has been executed. Additionally, if any supervision or context c is available at the time of synthetic gradient computation, the synthetic gradient\n",
    "model can take this as an extra input, $δ^{ˆ} = M(h,c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://norman3.github.io/papers/images/synthetic_gradients/f03.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Synthetic Gradient for Recurrent Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://sanghyukchun.github.io/images/post/89-1.PNG\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms.google.com.a.appspot.com/images/3-7.width-1500.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.wildml.com/wp-content/uploads/2015/10/rnn-bptt-with-gradients.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### truncated BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms.google.com.a.appspot.com/images/3-8.width-1500.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPTT using DNI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms.google.com.a.appspot.com/images/3-9.width-1500.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Figure 3. (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This amounts to taking the infinitely unrolled RNN as the full neural network $F_{1}^∞$, and chunking it into an infinite number of sub-networks where the recurrent core is unrolled for T steps, giving $F_{t}^{t+T−1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scheme can be implemented very efficiently by exploiting the recurrent nature of the network, as shown in Fig. 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Experiments\n",
    "* 3.1 Feed-Forward Networks\n",
    "* 3.2 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Feed-Forward Networks\n",
    "* Every layer DNI\n",
    "* Sparse Updates\n",
    "* Complete Unlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every layer DNI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Unlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Recurrent Neural Networks\n",
    "* Copy and Repeat Copy\n",
    "* Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy and Repeat Copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap18.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multi-Network System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] (deepmind original paper) Decoupled Neural Interfaces using Synthetic Gradients - https://arxiv.org/pdf/1608.05343.pdf\n",
    "* [2] (deepmind blog) Decoupled Neural Interfaces using Synthetic Gradients - https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/\n",
    "* [3] (모두연) Decoupled Neural Interfaces using Synthetic Gradients - https://norman3.github.io/papers/docs/synthetic_gradients\n",
    "* [4] (code) Image classification with synthetic gradient in tensorflow - https://github.com/andrewliao11/DNI-tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
